---
title: 'Increasing discovery rates in preclinical research through optimised statistical decision criteria'
csl: nature-methods.csl
indent: yes
output: bookdown::pdf_document2
toc: false
linestretch: 1.5
fontsize: 11pt
header-includes:
       - \usepackage{subfig}
         \usepackage{ulem}
abstract: \singlespacing Brief Communications begin with a brief unreferenced abstract (3 sentences, no more than 70 words),
bibliography: refs_repro.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
# options(tinytex.verbose = TRUE)
```

```{r libraries, echo = FALSE, message = FALSE, warning = FALSE}
library(bookdown)
library(knitr)
library(tidyverse)
```


```{r data, include = FALSE, cache = FALSE}
load("all_outcomes_init_samp_size_10.RData")
load("exploratory_data_SESOI_Carneiro_10.RData")
load("exploratory_data_SESOI_Szucs_10.RData")
load("exploratory_data_significance_Carneiro_10.RData")
load("exploratory_data_significance_Szucs_10.RData")
```

**Main**

\ \ \ \ Preclinical research is essential for identifying promising interventions and to generate robust evidence to support translation to humans. To achieve these goals, experiments are conducted in two different operating modes [@kimmelman2014distinguishing]. Early-stage preclinical experiments are *exploratory* with the aim to discover potentially effective interventions and generate hypotheses. These are tested at a later stage under more strict conditions in *confirmatory* mode [@landis2012call].

This sequential approach will increase the likelihood that generated evidence is robust and potentially reduce translational failure [@dirnagl2016thomas; @kimmelman2014distinguishing; @landis2012call;@mogil2017no][ADD elife paper here and cut landis and kimmelman]. Before conducting confirmatory experiments, researchers need to decide which exploratory results should be contested. Commonly, such a decision is based on a statistical test and associated *p*-value threshold in an exploratory study [do we have a good citation here?]. Here, we argue that this standard approach is detrimental to the goal to actually identify efficacious treatments. We propose an alternative that is, mimicking clinical trials, focused on a smallest effect size of interest. We show through simulations that this will increase the probability that a confirmatory study identifies a true effect, i.e. the positive predictive value.[perhaps cite Ionanidis 2005 here]. 
[and will increaeGiven that prior probabilities at this stage of discovery are low and sample sizes are small [@macleod2008evidence; @howells2014bringing], the *p*-value might prematurely discard potentially promising interventions [@dirnagl2020resolving].]

Our approach is based on the notion that exploration and confirmation fulfill different roles in scientific discovery. Exploration detects via sensitive tests potentially relevant effects among many hypotheses that have a low prior probability of being true (in the following \textit{pre-study odds}). [@bonapersona2019repair; @dirnagl2020resolving]. As more sensitive criteria invite more false positive results, confirmation must aim at reducing false positives to ensure that only true effects are carried forward to subsequent testing. [To increase power and safeguard reliability of results, sample sizes must be increased when switching from exploratory to confirmatory mode. suggest delete] To complicate matters, ethical, time, and budget constraints limit degrees of freedom in experimental design. Consequently, to prevent false negative results in exploration and to reduce false positives during confirmation, it is necessary to devise strategies to move from exploration to confirmation that meet these complementary goals.

Specifically, the likelihood of false negative and false positive outcomes needs to be balanced against the number of animals. [Perhaps can give a small example here if space allows] 

To this end, we simulated two preclinical research trajectories each comprising an exploratory study and a first confirmatory study (Figure 1?). 

```{r experiments proceeding, echo = FALSE, results = "hide"}

exploratory_data_sig_Carneiro_10_0.05 <-
  exploratory_data_sig_Carneiro_10 %>% 
  filter(pval_threshold == 0.05) %>% 
  filter(selection_sig == 1) %>% 
  #filter(effect < 0) %>% 
  summarise(N = n())

exploratory_data_sig_Carneiro_10_0.05$N / 100
  
exploratory_data_Carneiro_10_0.5 <-
  exploratory_data_Carneiro_10 %>% 
  filter(SESOI == 0.5) %>% 
  filter(selection_equiv == 1) %>%
  # filter(effect < 0) %>% 
  summarise(N = n())

exploratory_data_Carneiro_10_0.5$N / 100

exploratory_data_Carneiro_10_1.0 <-
  exploratory_data_Carneiro_10 %>% 
  filter(SESOI == "1.0") %>% 
  filter(selection_equiv == 1) %>% 
  summarise(N = n())

exploratory_data_Carneiro_10_1.0$N / 100

exploratory_data_sig_Szucs_10_0.05 <-
  exploratory_data_sig_Szucs_10 %>% 
  filter(pval_threshold == 0.05) %>% 
  filter(selection_sig == 1) %>% 
  # filter(effect < 0) %>%
  summarise(N = n())

exploratory_data_sig_Szucs_10_0.05$N / 100
  
exploratory_data_Szucs_10_0.5 <-
  exploratory_data_Szucs_10 %>% 
  filter(SESOI == 0.5) %>% 
  filter(selection_equiv == 1) %>%
  # filter(effect < 0) %>% 
  summarise(N = n())

exploratory_data_Szucs_10_0.5$N / 100

exploratory_data_Szucs_10_1.0 <-
  exploratory_data_Szucs_10 %>% 
  filter(SESOI == "1.0") %>% 
  filter(selection_equiv == 1) %>% 
  summarise(N = n())

exploratory_data_Szucs_10_1.0$N / 100
```


We based our simulations on two published empirical effect size distributions.  reflecting an *optimistic* (high pre-study odds) and a *pessimistic* (low pre-study odds) scenario (Supplementary Figure 1). After an initial exploratory study, a decision criterion identified experiments that should move from exploratory to confirmatory mode. One trajectory (standard) employed the conventional significance threshold ($\alpha$ = .05) for this decision. The second trajectory (SESOI) used a more lenient threshold based on an *a priori* determined smallest effect size of interest (SESOI), a practice also employed in clinical trials [CITATION]. [copied from below] For this, we estimated the effect size of each exploratory study and associated 95 percent confidence interval (CI). We examined whether this CI covered our SESOI (Cohen's *d* of 0.5 and 1.0, respectively).

\sout{In the standard trajectory, under optimistic pre-study odds, `r exploratory_data_sig_Szucs_10_0.05$N / 100` percent of experiments met the criterion *p* $\leq$ .05. In the pessimistic scenario, `r exploratory_data_sig_Carneiro_10_0.05$N / 100` percent of experiments had a *p*-value $\leq$ .05. Effect sizes of experiments that proceeded to confirmation reveals that the conventional significance threshold is a conservative filter. Many effect sizes $\neq$ 0 are eliminated and not further investigated (Figure \@ref(fig:histograms)a--b). This demonstrates that the conventional significance threshold is not useful to screen for potentially meaningful effects at the early stages of drug discovery even if pre-study odds are high.}

\emph{I rephrased the entire previous paragraph for readability. The main take home message in this rewrite is that you currently do not guide the reader through the results. You state the results but do not imply an interpretation.}

As expected, more experiments transitioned to the confirmatory phase in the SESOI trajectory compared to the standard trajectory. This was true for both the optimistic and pessimistic pre-study odds (PUT STATS HERE). This was also reflected in effect sizes where many effect sizes $\neq$ 0 are eliminated and not further investigated (Figure \@ref(fig:histograms)a--b). This demonstrates that the conventional significance threshold is not useful to screen for potentially meaningful effects at the early stages of drug discovery even if pre-study odds are high. Contrary, the range of effect size estimates in the SESOI trajectory that proceeded to confirmation shifted[shifted where?]  and included less extreme values, as well as more values closer to zero (Figure \@ref(fig:histograms)c--f). This decision criterion thus reduces false negatives compared to the conventional significance threshold.


\sout{In the SESOI trajectory, we estimated the exploratory effect size and 95 percent confidence interval (CI) around that estimate. We examined whether the CI covered our SESOI (Cohen's *d* of 0.5 and 1.0, respectively). If this was the case for an experiment, it advanced to confirmatory mode. Importantly, we did not consider the *p*-value additionally. Applying this decision criterion resulted in `r exploratory_data_Szucs_10_0.5$N / 100` and `r exploratory_data_Szucs_10_1.0$N / 100` percent of experiments moving to confirmation in case of the optimistic distribution and `r exploratory_data_Carneiro_10_0.5$N / 100` and `r exploratory_data_Carneiro_10_1.0$N / 100` percent for the pessimistic distribution based on a SESOI of 0.5 and 1.0, respectively. Compared to the conventional significance threshold, the range of effect size estimates that proceeded to confirmation shifted and included less extreme values, as well as more values closer to zero (Figure \@ref(fig:histograms)c--f). This decision criterion meets the goal of preventing false negatives better than the conventional significance threshold.}

```{r histograms, echo = FALSE, fig.cap = 'Samples (n = 10000) drawn from the two empirical effect size distributions (left: optimistic, right: pessimistic). The panels show the underlying effect sizes that were detected using one of the two decision criteria. Orange shaded bars (panels (a) and (b)) indicate those effect sizes that were identified for replication using the conventional significance threshold (p = .05). Blue shaded bars indicate effect sizes that were selected using a SESOI of 0.5 (panels (c) and (d)) or 1.0 (panels (e) and (f)), respectively. Note that in (a), (c), and (e) 16 values $>$ 10 were removed in order to display the distribution.', fig.subcap = c('', ''), out.width = '.50\\linewidth', fig.ncol = 2, fig.align = 'center'}

exploratory_data_sig_Szucs_10 <-
  exploratory_data_sig_Szucs_10 %>% 
  filter(pval_threshold == 0.05) %>% 
  filter(ES_true <= 10)
  
ES_histrogram_Szucs_selected_sig <-
  ggplot(data = exploratory_data_sig_Szucs_10, 
       aes(x = ES_true, fill = factor(selection_sig))) +
  geom_histogram(bins = 90, color = "black",
                 size = 0.3, alpha = 0.8) +
  labs(#x = "",
       x = expression(paste("Cohen's ", italic("d"))),
       y = "Frequency",
       fill = "Selected for \nreplication") +
  scale_fill_manual(breaks = c("0", "1"),
                    labels = c("no",
                               "yes"),
                    values = c("white", "#E69F00")) +
  theme_bw() +
  theme(axis.title.x = element_text(size = 16)) +
  theme(axis.title.y = element_text(size = 16)) +
  theme(axis.text.x = element_text(size = 15, colour = "black")) +
  theme(axis.text.y = element_text(size = 15, colour = "black")) +
  theme(legend.position = "none")

ES_histrogram_Szucs_selected_sig

exploratory_data_sig_Carneiro_10 <-
  exploratory_data_sig_Carneiro_10 %>% 
  filter(pval_threshold == 0.05)

ES_histrogram_Carneiro_selected_sig <-
  ggplot(data = exploratory_data_sig_Carneiro_10, 
       aes(x = ES_true, fill = factor(selection_sig))) +
  geom_histogram(bins = 90, color = "black",
                 size = 0.3, alpha = 0.8) +
  labs(#x = "",
       x = expression(paste("Cohen's ", italic("d"))),
       y = "Frequency",
       fill = "Selected for \nreplication") +
  scale_fill_manual(breaks = c("0", "1"),
                    labels = c("no",
                               "yes"),
                    values = c("white", "#E69F00")) +
  theme_bw() +
  theme(axis.title.x = element_text(size = 16)) +
  theme(axis.title.y = element_text(size = 16)) +
  theme(axis.text.x = element_text(size = 15, colour = "black")) +
  theme(axis.text.y = element_text(size = 15, colour = "black")) +
  theme(legend.position = "none")

ES_histrogram_Carneiro_selected_sig

exploratory_data_Szucs_10_0.5 <-
  exploratory_data_Szucs_10 %>% 
  filter(SESOI == 0.5) %>% 
  filter(ES_true <= 10)

ES_histrogram_Szucs_selected_SESOI_0.5 <-
  ggplot(data = exploratory_data_Szucs_10_0.5, 
       aes(x = ES_true, fill = factor(selection_equiv))) +
  geom_histogram(bins = 90, color = "black",
                 size = 0.3, alpha = 0.8) +
  labs(#x = "",
       x = expression(paste("Cohen's ", italic("d"))),
       y = "Frequency",
       fill = "Selected for \nreplication") +
  scale_fill_manual(breaks = c("0", "1"),
                    labels = c("no",
                               "yes"),
                    values = c("white", "#0072B2")) +
  theme_bw() +
  theme(axis.title.x = element_text(size = 16)) +
  theme(axis.title.y = element_text(size = 16)) +
  theme(axis.text.x = element_text(size = 15, colour = "black")) +
  theme(axis.text.y = element_text(size = 15, colour = "black")) +
  theme(legend.position = "none")

ES_histrogram_Szucs_selected_SESOI_0.5


exploratory_data_Carneiro_10_0.5 <-
  exploratory_data_Carneiro_10 %>% 
  filter(SESOI == 0.5)

ES_histrogram_Carneiro_selected_SESOI_0.5 <-
  ggplot(data = exploratory_data_Carneiro_10_0.5, 
       aes(x = ES_true, fill = factor(selection_equiv))) +
  geom_histogram(bins = 90, color = "black",
                 size = 0.3, alpha = 0.8) +
  labs(#x = "",
       x = expression(paste("Cohen's ", italic("d"))),
       y = "Frequency",
       fill = "Selected for \nreplication") +
  scale_fill_manual(breaks = c("0", "1"),
                    labels = c("no",
                               "yes"),
                    values = c("white", "#0072B2")) +
  theme_bw() +
  theme(axis.title.x = element_text(size = 16)) +
  theme(axis.title.y = element_text(size = 16)) +
  theme(axis.text.x = element_text(size = 15, colour = "black")) +
  theme(axis.text.y = element_text(size = 15, colour = "black")) +
  theme(legend.position = "none")

ES_histrogram_Carneiro_selected_SESOI_0.5

exploratory_data_Szucs_10_1.0 <-
  exploratory_data_Szucs_10 %>% 
  filter(SESOI == "1.0") %>% 
  filter(ES_true <= 10)

ES_histrogram_Szucs_selected_SESOI_1.0 <-
  ggplot(data = exploratory_data_Szucs_10_1.0, 
       aes(x = ES_true, fill = factor(selection_equiv))) +
  geom_histogram(bins = 90, color = "black",
                 size = 0.3, alpha = 0.8) +
  labs(#x = "",
       x = expression(paste("Cohen's ", italic("d"))),
       y = "Frequency",
       fill = "Selected for \nreplication") +
  scale_fill_manual(breaks = c("0", "1"),
                    labels = c("no",
                               "yes"),
                    values = c("white", "#0072B2")) +
  theme_bw() +
  theme(axis.title.x = element_text(size = 16)) +
  theme(axis.title.y = element_text(size = 16)) +
  theme(axis.text.x = element_text(size = 15, colour = "black")) +
  theme(axis.text.y = element_text(size = 15, colour = "black")) +
  theme(legend.position = "none")

ES_histrogram_Szucs_selected_SESOI_1.0


exploratory_data_Carneiro_10_1.0 <-
  exploratory_data_Carneiro_10 %>% 
  filter(SESOI == "1.0")

ES_histrogram_Carneiro_selected_SESOI_1.0 <-
  ggplot(data = exploratory_data_Carneiro_10_1.0, 
       aes(x = ES_true, fill = factor(selection_equiv))) +
  geom_histogram(bins = 90, color = "black",
                 size = 0.3, alpha = 0.8) +
  labs(#x = "",
       x = expression(paste("Cohen's ", italic("d"))),
       y = "Frequency",
       fill = "Selected for \nreplication") +
  scale_fill_manual(breaks = c("0", "1"),
                    labels = c("no",
                               "yes"),
                    values = c("white", "#0072B2")) +
  theme_bw() +
  theme(axis.title.x = element_text(size = 16)) +
  theme(axis.title.y = element_text(size = 16)) +
  theme(axis.text.x = element_text(size = 15, colour = "black")) +
  theme(axis.text.y = element_text(size = 15, colour = "black")) +
  theme(legend.position = "none")

ES_histrogram_Carneiro_selected_SESOI_1.0
```

```{r ES data, include = FALSE, cache = FALSE}
load("ES_data_Szucs.RData")
load("ES_data_Carneiro.RData")
```

```{r ES distribution, echo = FALSE, results = "hide"}
min(ES_data_Szucs$D)
max(ES_data_Szucs$D)

sum(ES_data_Szucs$D > 20)

median(ES_data_Szucs$D)
mean(ES_data_Szucs$D)

names(ES_data_Carneiro)[1] <- "D"

ES_data_Carneiro$D <- ifelse(ES_data_Carneiro$D < 0, -ES_data_Carneiro$D, -ES_data_Carneiro$D)

min(ES_data_Carneiro$D)
max(ES_data_Carneiro$D)
median(ES_data_Carneiro$D)
mean(ES_data_Carneiro$D)

ES_data_Carneiro <-
  ES_data_Carneiro %>% 
  mutate(distribution = "Pessimistic")

ES_Szucs <-
  ES_data_Szucs %>% 
  select(D) %>% 
  filter(D <= 20) %>% 
  mutate(distribution = "Optimistic")

ES_data <- bind_rows(ES_Szucs, ES_data_Carneiro)
```

[Please adjust the following paragraphs in the same way I have adjusted the one above. State the result and give descriptive stats in brackets. As it is this is way too long and does not read smoothly.]

From the experiments that were identified for confirmation only those that showed exploratory effect sizes in favor of the treatment were further investigated. In a second step, we calculated the sample size for a first confirmatory study. In the standard trajectory this was done via a standard power analysis using the initial exploratory effect size. The SESOI trajectory used again a pre-defined smallest effect size of interest (SESOI).

In the standard trajectory, this resulted in a mean number of `r round(outcomes_all_distributions_init_samp_size_10$mean_N[7], 2)` (SD = `r round(outcomes_all_distributions_init_samp_size_10$mean_N[7] - outcomes_all_distributions_init_samp_size_10$mean_N_min[7], 2)`) animals in the optimistic, and `r round(outcomes_all_distributions_init_samp_size_10$mean_N[3], 2) ` (SD = `r round(outcomes_all_distributions_init_samp_size_10$mean_N[3] - outcomes_all_distributions_init_samp_size_10$mean_N_min[3], 2)`) in the pessimistic scenario (Figure \@ref(fig:results)a). These small numbers reflect the large effect sizes that passed on to confirmation and were the basis for sample size calculation (Supplementary Figure 2a--b).

In the SESOI trajectory, the number of animals varied with the SESOI that was chosen. For an SESOI of 1.0, `r round(outcomes_all_distributions_init_samp_size_10$mean_N[1])` animals were needed in the confirmatory study in both the optimistic and pessimistic scenario. If the SESOI was 0.5, animal numbers increased to `r round(outcomes_all_distributions_init_samp_size_10$mean_N[2])` (Figure \@ref(fig:results)a).

We further calculated the positive predictive value (PPV), false positive rate (FPR), and false negative rate (FNR) across both trajectories. The positive predictive value (PPV) of a study is the post-study probability that a positive finding which is based on statistical significance reflects a true effect [@ioannidis2005most]. The PPV is calculated from the pre-study odds, as well as the sensitivity and specificity of the test. In our study, pre-study odds of an effect of a given size (0.5 and 1.0, respectively) were determined by the empirical effect size distributions. If evidence for an initial claim is strengthened throughout the preclinical research trajectory, we would observe an increased PPV compared to pre-study odds.

In the optimistic scenario, the pre-study odds were `r round(outcomes_all_distributions_init_samp_size_10$prev_pop[6], 2)` and `r round(outcomes_all_distributions_init_samp_size_10$prev_pop[5], 2)` for SESOI of 0.5 and 1.0, respectively. In the pessimistic scenario pre-study odds were `r round(outcomes_all_distributions_init_samp_size_10$prev_pop[2], 2)` and `r round(outcomes_all_distributions_init_samp_size_10$prev_pop[1], 2)`, respectively. 

Across the standard trajectory, the PPV drops below pre-study odds in both scenarios (Figure \@ref(fig:results)b). After the within-lab replication, the PPV is `r round(outcomes_all_distributions_init_samp_size_10$PPV_pop_prev[8], 2)` and  `r round(outcomes_all_distributions_init_samp_size_10$PPV_pop_prev[7], 2)` in the optimistic scenario for SESOI of 0.5 and 1.0, respectively. In the pessimistic scenario, the PPV is `r round(outcomes_all_distributions_init_samp_size_10$PPV_pop_prev[4], 2)` and `r round(outcomes_all_distributions_init_samp_size_10$PPV_pop_prev[3], 2)`.

In the SESOI trajectory, employing a SESOI at both stages along the decision-making process elevates the PPV above pre-study odds (Figure \@ref(fig:results)b). Given a SESOI of 0.5 and 1.0, respectively, the PPV is `r round(outcomes_all_distributions_init_samp_size_10$PPV_pop_prev[6], 2)` and `r round(outcomes_all_distributions_init_samp_size_10$PPV_pop_prev[5], 2)` in the optimistic scenario, and `r round(outcomes_all_distributions_init_samp_size_10$PPV_pop_prev[2], 2)` and `r round(outcomes_all_distributions_init_samp_size_10$PPV_pop_prev[1], 2)` in the pessimistic scenario.

Across the standard trajectory, given the optimistic scenario, the FPR was `r round(outcomes_all_distributions_init_samp_size_10$FPR[8], 2)` and `r round(outcomes_all_distributions_init_samp_size_10$FPR[7], 2)` for SESOI of 0.5 and 1.0, respectively. In the pessimistic scenario, the FPR was `r round(outcomes_all_distributions_init_samp_size_10$FPR[4], 3)` and `r round(outcomes_all_distributions_init_samp_size_10$FPR[3], 2)` for SESOI of 0.5 and 1.0, respectively (Figure \@ref(fig:results)c).

Across the SESOI trajectory, the FPR increased to `r round(outcomes_all_distributions_init_samp_size_10$FPR[6], 2)` and `r round(outcomes_all_distributions_init_samp_size_10$FPR[5], 2)` in the optimistic scenario for SESOI of 0.5 and 1.0. Given the pessimistic scenario, the FPR was `r round(outcomes_all_distributions_init_samp_size_10$FPR[2], 2)` and `r round(outcomes_all_distributions_init_samp_size_10$FPR[1], 2)` for SESOI set to 0.5 and 1.0 (Figure \@ref(fig:results)c). 

Across the standard trajectory, given the optimistic scenario, the FNR was `r round(outcomes_all_distributions_init_samp_size_10$FNR[8], 2)` and `r round(outcomes_all_distributions_init_samp_size_10$FNR[7], 2)` for SESOI set to 0.5 and 1.0, respectively.  In the pessimistic scenario, the FNR was `r round(outcomes_all_distributions_init_samp_size_10$FNR[4], 2)` and `r round(outcomes_all_distributions_init_samp_size_10$FNR[3], 2)` for SESOI of 0.5 and 1.0, respectively (Figure \@ref(fig:results)d).

Across the SESOI trajectory, the FNR decreased to `r round(outcomes_all_distributions_init_samp_size_10$FNR[6], 2)` and `r round(outcomes_all_distributions_init_samp_size_10$FNR[5], 2)` in the optimistic scenario for SESOI set to 0.5 and 1.0. Given the pessimistic scenario, the FNR was `r round(outcomes_all_distributions_init_samp_size_10$FNR[2], 2)` and `r round(outcomes_all_distributions_init_samp_size_10$FNR[1], 2)` for SESOI of 0.5 and 1.0 (Figure \@ref(fig:results)d).

```{r, echo = FALSE}

outcomes_all_distributions_init_samp_size_10$traject_SESOI <- 
  interaction(outcomes_all_distributions_init_samp_size_10$trajectory, outcomes_all_distributions_init_samp_size_10$SESOI)

plot_data_N <-
  outcomes_all_distributions_init_samp_size_10 %>% 
  filter(init_sample_size == 10) %>% 
  filter(traject_SESOI != "T1.1") %>% 
  group_by(distribution, SESOI)

plot_data <-
  outcomes_all_distributions_init_samp_size_10 %>% 
  filter(init_sample_size == 10) %>% 
  group_by(distribution, SESOI)
```

```{r results, echo = FALSE, message = FALSE, fig.cap = '(a) Number of animals needed in the first confirmatory study. In the standard trajectory, sample sizes are low, as they are based on large exploratory effect sizes. Error bars represent standard deviations. In case of trajectories using a SESOI, the number of animals is fixed. Note that all sample sizes displayed and reported in the text are the number of animals needed in *each* group (control and intervention).(b) Positive predictive value across trajectory. Dashed lines indicate pre-study odds based on empirical effect size distributions.', fig.subcap = c('', ''), out.width = '.50\\linewidth', fig.ncol = 2, fig.align = 'center'}

plot_mean_N <-
  ggplot(data = plot_data_N,
         aes(x = factor(traject_SESOI), y = mean_N, fill = factor(trajectory))) +
  geom_bar(stat = "identity", position = "dodge",
           size = .3,
           color = "black",
           alpha = 0.8) +
  geom_errorbar(aes(ymax = mean_N_max, ymin = mean_N_min), width = 0.1,
                position = position_dodge(width = 0.9)) +
  facet_grid(~ distribution) +
  # ggtitle("Mean number of animals for each trajectory") +
  labs(x = " ", y = "Mean # of animals in replication",
       fill = "Trajectory") +
  scale_x_discrete(labels = c("Standard", "SESOI = 0.5", "SESOI = 1.0")) +
  scale_fill_manual(labels = c("Standard", "SESOI"), values = c("#E69F00", "#0072B2")) + 
  theme_bw() +
  theme(axis.title.x = element_text(size = 16)) +
  theme(axis.title.y = element_text(size = 16)) +
  theme(axis.text.x = element_text(size = 14, colour = "black",
                                   angle = 45, hjust = 1)) +
  theme(axis.text.y = element_text(size = 15, colour = "black")) +
  # theme(axis.title.x = element_text(size = 14)) +
  # theme(axis.title.y = element_text(size = 14)) +
  # theme(axis.text.x = element_text(size = 12, colour = "black")) +
  # theme(axis.text.y = element_text(size = 12, colour = "black")) +
  theme(strip.text.x = element_text(size = 14, colour = "black")) +
  theme(strip.background = element_rect(fill = "white", color = "black")) +
  theme(legend.title = element_text(size = 13, face = "bold")) +
  theme(legend.text = element_text(size = 12)) +
  theme(legend.position = "none")
  # theme(title = element_text(size = 15))

plot_mean_N

facet_names <- 
  c("0.5" = "SESOI = 0.5",
    "1" = "SESOI = 1.0")

plot_PPV <-
  ggplot(data = plot_data, 
         aes(x = factor(trajectory), 
             y = PPV_pop_prev,
             color = trajectory)) +
  geom_point(size = 3) +
  facet_grid(SESOI ~ distribution, labeller = labeller(.rows = facet_names)) +
  labs(x = "Trajectory", y = "Positive predictive value") +
  scale_x_discrete(labels = c("Standard", "SESOI")) +
  scale_color_manual(breaks = c("T1", "T2"),
                    values = c("#E69F00", "#0072B2")) +
  theme_bw() +
  theme(axis.title.x = element_text(size = 16)) +
  theme(axis.title.y = element_text(size = 16)) +
  theme(axis.text.x = element_text(size = 15, colour = "black")) +
  theme(axis.text.y = element_text(size = 15, colour = "black")) +
  theme(strip.text.x = element_text(size = 14, colour = "black")) +
  theme(strip.text.y = element_text(size = 14, colour = "black")) +
  theme(strip.background = element_rect(fill = "white", color = "black")) +
  theme(legend.position = "none")

hlines <- data.frame(pre_study_odds = c(plot_data$Prevalence[1], plot_data$Prevalence[2],
                                        plot_data$Prevalence[5], plot_data$Prevalence[6]), 
                     distribution   = c(rep(plot_data$distribution[1], 2), 
                                        rep(plot_data$distribution[5], 2)),
                     SESOI          = rep(c("1", "0.5"))) 

plot_PPV <- 
  plot_PPV + 
  geom_hline(data = hlines, 
             aes(yintercept = pre_study_odds),
             color = "black", lty = 2, size = .5)

plot_PPV

facet_names <- c(
  "0.5" = "SESOI = 0.5",
  "1" = "SESOI = 1.0")

plot_FPR <-
  ggplot(data = plot_data, 
         aes(x = factor(trajectory), 
             y = FPR,
             color = trajectory)) +
  geom_point(size = 3, alpha = .8) +
  facet_grid(SESOI ~ distribution, labeller = labeller(.rows = facet_names)) +
  labs(x = "Trajectory", y = "False positive rate") +
  scale_x_discrete(labels = c("Standard", "SESOI")) +
  scale_color_manual(breaks = c("T1", "T2"),
                    values = c("#E69F00", "#0072B2")) +
  theme_bw() +
  theme(axis.title.x = element_text(size = 16)) +
  theme(axis.title.y = element_text(size = 16)) +
  theme(axis.text.x = element_text(size = 15, colour = "black")) +
  theme(axis.text.y = element_text(size = 15, colour = "black")) +
  theme(strip.text.x = element_text(size = 14, colour = "black")) +
  theme(strip.text.y = element_text(size = 14, colour = "black")) +
  theme(strip.background = element_rect(fill = "white", color = "black")) +
  theme(legend.position = "none")

plot_FPR

facet_names <- c(
  "0.5" = "SESOI = 0.5",
  "1" = "SESOI = 1.0")

plot_FNR <-
  ggplot(data = plot_data, 
         aes(x = factor(trajectory), 
             y = FNR,
             color = trajectory)) +
  geom_point(size = 3, alpha = .8) +
  facet_grid(SESOI ~ distribution, labeller = labeller(.rows = facet_names)) +
  labs(x = "Trajectory", y = "False negative rate") +
  scale_x_discrete(labels = c("Standard", "SESOI")) +
  scale_color_manual(breaks = c("T1", "T2"),
                    values = c("#E69F00", "#0072B2")) +
  theme_bw() +
  theme(axis.title.x = element_text(size = 16)) +
  theme(axis.title.y = element_text(size = 16)) +
  theme(axis.text.x = element_text(size = 15, colour = "black")) +
  theme(axis.text.y = element_text(size = 15, colour = "black")) +
  theme(strip.text.x = element_text(size = 14, colour = "black")) +
  theme(strip.text.y = element_text(size = 14, colour = "black")) +
  theme(strip.background = element_rect(fill = "white", color = "black")) +
  theme(legend.position = "none")

plot_FNR
```

Overall, our simulation shows that current practice reflected by the standard trajectory does not meet the complementary goals of exploration and confirmation along a preclinical research trajectory. In the standard trajectory, the switch from exploratory to confirmatory mode eliminates numerous potentially meaningful effects. This shortcoming is also reflected in the PPV where only 40 out 100 significant results reflect an underlying true effect (compared to 73/100 in the SESOI trajectory). In light of our findings, we advise a rejection of the standard approach. [After shortening the results part significantly here we can drop a few lines more on how to apply this. Adjust the SESOI to the feasibility in the lab. This will raise awareness for the limitations of the study! We could also hint at the number of initial animals and how these influence our results. We should also emphasise the deviation from one experiment one result practice but towards a systemic investigation of true effects. Also hint at the possibility for multi center trials and how much costs are saved downstream].We present here an easily applicable alternative to current practice. Optimizing decision criteria and sample size calculations by employing a SESOI increases chances to detect true effects while keeping the number of animals reasonably low.




\newpage

**Methods**

\scriptsize

\ \ \ \ **Simulation.** We explored different approaches to perform preclinical animal experiments via simulations. To this end, we modeled a simplified preclinical research trajectory from the exploratory stage to the results of a first confirmatory study (within-lab replication; Figure \@ref(fig:trajectory)) (\textcolor{red}{make new figure and include two trajectories in MS and all 4 in Supplement}). Along the trajectory, there are different ways to increase the probability of not missing potentially meaningful effects. After an initial exploratory study, a first decision identifies experiments for replication. In our simulation, we employed two different decision criteria that indicate when one should move from the exploratory to confirmatory mode. If a decision has been made to replicate an initial study, we applied two approaches to determine the sample size for a replication study (smallest effect size of interest (SESOI) and standard power analysis), as outlined in detail below.


\ \ \ \ *Empirical effect size distributions.* Simulations were based on empirical effect size distributions from the recently published literature  [@szucs2017empirical; @carneiro2018effect]. This enabled us to determine the prior probability (pre-study odds) of a certain alternative hypothesis ($H_{1}$) which we defined as an effect of a given size (e.g. a Cohen's *d* of 0.5).

The distribution of effect sizes extracted from Szucs & Ioannidis (2017) [@szucs2017empirical] contains 26841 effect sizes from the cognitive neuroscience and psychology literature published between January 2011 and August 2014. All effect sizes are calculated as the standardized difference in means (Cohen's *d*). Effect size estimates range from `r round(min(ES_data_Szucs$D), 2)` to `r round(max(ES_data_Szucs$D), 2)`, and have a median of `r round(median(ES_data_Szucs$D), 2)`. As the pre-study odds of a medium effect of 0.5 are rather large (`r round(outcomes_all_distributions_init_samp_size_10$prev_pop[6], 2)`), we will refer to this distribution as *optimistic*.

Our second empirical distribution was taken from Carneiro et al.'s (2018) [@carneiro2018effect] study which systematically examined effect sizes in the rodent fear conditioning literature. Effect sizes were extracted from 410 experiments published in PubMed in 2013. The publication included a data file containing all extracted effect sizes. After removing missing values, the data set consisted of 336 effect sizes, again, calculated as Cohen's *d*. The effect sizes range from `r round(min(ES_data_Carneiro$D), 2)` to `r round(max(ES_data_Carneiro$D), 2)`, and have a median of `r round(median(ES_data_Carneiro$D), 2)`. The prior probability of observing an effect of 0.5 is `r round(outcomes_all_distributions_init_samp_size_10$prev_pop[2], 2)`. We will therefore refer to this distribution as *pessimistic*.

\ \ \ \ *Exploratory mode.* From each of the two distributions, we drew 10000 samples of effect sizes from which we created 10000 study data sets. Each data set comprised data of two groups consisting of ten experimental units each drawn from a normal distribution. We chose a number of ten EUs based on reported sample sizes in preclinical studies [@howells2014bringing]. Our simulated design mimics a comparison between two groups where one receives an intervention and the other functions as a control group. The study data sets are compared using a two-sided two-sample *t*-test. From these exploratory study results, we extracted the *p*-values and 95 percent confidence intervals (CI). We then employed two different criteria based on the *p*-value or 95 percent CI, respectively, to decide whether to continue to a confirmatory mode.

\ \ \ \ *Decision criteria to proceed to replication.* The first decision criterion employs the conventional significance threshold  ($\alpha$ = .05) to decide whether to replicate an exploratory study. If a *p*-value extracted from a two-sided two-sample *t*-test is $\leq$ .05, this study will proceed to confirmation. If not, the trajectory is terminated after the exploratory study. We chose this decision criterion as our reference, as this is what we consider to be current practice. 

As an alternative to this approach, we propose to set a smallest effect size of interest (SESOI) and examine whether the 95 percent CI around the exploratory effect size estimate covers this SESOI. A SESOI is the effect size that the researcher based their domain knowledge and given practical constraints considers biologically and clinically meaningful [@lakens2018equivalence]. In our simulation, we used 0.5 and 1.0 as SESOI. This approach emphasizes the importance of effect sizes rather than statistical significance to evaluate an intervention's effect. Further, we expected this approach to be more lenient than statistical significance (at least if the significance threshold was set at $\alpha$ = .05) and to allow a broader range of effect sizes to pass on to be further investigated.

\ \ \ \ *Approaches to determine sample size for replication.* Once the decision to continue to confirmatory mode has been made, we employed two different approaches to determine the sample size for the first confirmatory study. After the exploratory study, we have an estimate of the direction of the effect. Only effect sizes that showed an effect in favor of the treatment were considered for further investigation. Thus, for the confirmatory study, a one-sided two-sample *t*-test was performed. 
In the standard trajectory, the desired power level for replication was set to .80, $\alpha$ was set to .05. To calculate the sample size given power and $\alpha$, we used the exploratory effect size estimate.
In the SESOI trajectory, we employed the same SESOI used as decision criterion earlier. Our SESOI was set such that the confirmatory study would have a power of .50 to detect an effect of this size. This power level was chosen to ensure that the likelihood of a false positive finding below the threshold determined by our SESOI is negligible. The aim during confirmation is to weed out false positives. 

\ \ \ \ *Confirmatory mode.* For each of the studies that met the decision criterion after the exploratory study (either *p* $\leq$ .05 or SESOI within the 95 percent CI of the exploratory effect size estimate), a confirmatory study was performed. The number of studies conducted varied with the decision criterion used and, in case of the criterion employing a SESOI, also with the SESOI (0.5 and 1.0). A confirmatory study was performed as a one-sided two-sample *t*-test, where the number of animals in each group was determined by the approach to calculate the sample size. For a confirmation to be considered "successful", the *p*-value had to be below the conventional significance threshold ($\alpha$ = .05).

\ \ \ \ *Outcome variables* We compared the two trajectories (standard and SESOI) regarding the number of experiments proceeding to confirmatory mode, number of animals needed in the confirmatory study, and positive predictive value (PPV), false positive rate (FPR), and false negative rate (FNR) across the trajectory.

\newpage

**References**
\ \ \ \ 
\singlespacing
